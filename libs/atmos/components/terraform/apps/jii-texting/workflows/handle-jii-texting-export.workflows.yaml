# This is a basic Workflow that takes the state code and determines if we should 
# execute the process-jii-to-text Workflow 

main:
  params: [event]
  steps:
    - decode_pubsub_message:
        assign:
          - base64: ${base64.decode(event.data.message.data)}
          - message: ${json.decode(base64)}
    - init:
        assign:
          - state_code: ${message.state_code}
          - cloud_run_service_url: ${sys.get_env("CLOUD_RUN_SERVICE_URL")}
          - project_id: ${sys.get_env("PROJECT_ID")}
          - current_date_str: ${text.substring(time.format(sys.now()), 0, 10)}
          - dry_run: ${sys.get_env("PROJECT_ID") != "recidiviz-dashboard-production"}
          - archive_bucket_id: ${sys.get_env("ARCHIVE_BUCKET_ID")}
          - etl_bucket_id: ${sys.get_env("ETL_BUCKET_ID")}
    - get_latest_logic_execution:
        call: http.get
        args:
          url: ${cloud_run_service_url + "/workflow-executions/latest/" + state_code}
          auth:
            type: OIDC
        result: latest_logic_execution_result
    - assign_latest_logic_execution:
        assign:
          - latest_logic_execution: ${latest_logic_execution_result.body.workflowExecution}
    - should_trigger_logic_execution:
        switch: 
          - condition: ${text.substring(latest_logic_execution.workflowExecutionTime, 0, 10) != current_date_str or latest_logic_execution == null}
            next: run_logic
        next: skip_logic
    - skip_logic:
        return: ${"Did not trigger logic execution"}
    - run_logic:
        call: googleapis.workflowexecutions.v1.projects.locations.workflows.executions.run
        args:
          workflow_id: process-jii-to-text
          location: us-central1
          project_id: ${project_id}
          argument: 
            dryRun: ${dry_run}
            stateCode: ${state_code}
          connector_params:
            skip_polling: true
        result: workflow
    - archive_etl_data:
        call: archive_files
        args:
          state_code: ${state_code}
          etl_bucket_id: ${etl_bucket_id}
          archive_bucket_id: ${archive_bucket_id}
          current_date_str: ${current_date_str}
    - finish:
        return: ${workflow}

archive_files:
  params: [state_code, etl_bucket_id, archive_bucket_id, current_date_str]
  steps:
    - get_objects_list:
        try:
          call: googleapis.storage.v1.objects.list
          args:
            bucket: ${etl_bucket_id}
            delimiter: /
            prefix: ${state_code + "/"}
          result: metric_export_objects_list
        except:
          as: e
          steps:
            - raise_get_objects_list_error:
                raise: ${e}
    - assign_metric_export_file_names:
        assign:
          - metric_export_file_names: []
    - get_file_names:
        for:
          value: metric_export_object
          index: _
          in: ${metric_export_objects_list.items}
          steps:
            - append_file_name:
                assign:
                  - metric_export_file_names: ${list.concat(metric_export_file_names, metric_export_object.name)}
    - archive_all_files:
        for:
          value: metric_export_file_name
          index: _
          in: ${metric_export_file_names}
          steps:
            - assign_object_name:
                assign:
                  - destination_object: ${current_date_str + "/" + metric_export_file_name}
            - copy_file:
                try:
                  call: googleapis.storage.v1.objects.copy
                  args:
                    destinationBucket: ${archive_bucket_id}
                    destinationObject: ${text.url_encode(destination_object)}
                    sourceBucket: ${etl_bucket_id}
                    sourceObject: ${text.url_encode(metric_export_file_name)}
                  result: etl_result
                except:
                  as: e
                  steps:
                    - raise_copy_error:
                        raise: ${e}
